{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##ML + STRUCTURED STREAMING\n",
    "\n",
    "#Costruzione di un modello ML e predizione in tempo reale sfruttando lo straming strutturato\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "#IMPORT\n",
    "import pyspark.ml.regression as rg\n",
    "import pyspark as pys \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql as sql\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.ml.feature as feat\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "import json \n",
    "\n",
    "\n",
    "import pyspark.ml.evaluation as ev"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "#Inizializzazione spark session/context e caricamento dataset\n",
    "\n",
    "#Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#Caricamento dataset da file CSV\n",
    "forest_path = 'forest_coverage_type.csv'\n",
    "\n",
    "forest=spark.read.csv(\n",
    "    forest_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(\"Schema DF: \")\n",
    "forest.printSchema()\n",
    "\n",
    "#Divisione fra train e test\n",
    "forest_train, forest_test=(\n",
    "    forest.randomSplit([0.7,0.3],seed=123)\n",
    ")\n",
    "\n",
    "#Esporto il dataset di test in formato CSV (in un unico file con repartition(1))\n",
    "forest_test.repartition(1).write.csv(\"forest_test.csv\",\"overwrite\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Schema DF: \n",
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_Rawah: integer (nullable = true)\n",
      " |-- Wilderness_Area_Neota: integer (nullable = true)\n",
      " |-- Wilderness_Area_Comanche: integer (nullable = true)\n",
      " |-- Wilderness_Area_CacheLaPoudre: integer (nullable = true)\n",
      " |-- Soil_type_2702: integer (nullable = true)\n",
      " |-- Soil_type_2703: integer (nullable = true)\n",
      " |-- Soil_type_2704: integer (nullable = true)\n",
      " |-- Soil_type_2705: integer (nullable = true)\n",
      " |-- Soil_type_2706: integer (nullable = true)\n",
      " |-- Soil_type_2717: integer (nullable = true)\n",
      " |-- Soil_type_3501: integer (nullable = true)\n",
      " |-- Soil_type_3502: integer (nullable = true)\n",
      " |-- Soil_type_4201: integer (nullable = true)\n",
      " |-- Soil_type_4703: integer (nullable = true)\n",
      " |-- Soil_type_4704: integer (nullable = true)\n",
      " |-- Soil_type_4744: integer (nullable = true)\n",
      " |-- Soil_type_4758: integer (nullable = true)\n",
      " |-- Soil_type_5101: integer (nullable = true)\n",
      " |-- Soil_type_5151: integer (nullable = true)\n",
      " |-- Soil_type_6101: integer (nullable = true)\n",
      " |-- Soil_type_6102: integer (nullable = true)\n",
      " |-- Soil_type_6731: integer (nullable = true)\n",
      " |-- Soil_type_7101: integer (nullable = true)\n",
      " |-- Soil_type_7102: integer (nullable = true)\n",
      " |-- Soil_type_7103: integer (nullable = true)\n",
      " |-- Soil_type_7201: integer (nullable = true)\n",
      " |-- Soil_type_7202: integer (nullable = true)\n",
      " |-- Soil_type_7700: integer (nullable = true)\n",
      " |-- Soil_type_7701: integer (nullable = true)\n",
      " |-- Soil_type_7702: integer (nullable = true)\n",
      " |-- Soil_type_7709: integer (nullable = true)\n",
      " |-- Soil_type_7710: integer (nullable = true)\n",
      " |-- Soil_type_7745: integer (nullable = true)\n",
      " |-- Soil_type_7746: integer (nullable = true)\n",
      " |-- Soil_type_7755: integer (nullable = true)\n",
      " |-- Soil_type_7756: integer (nullable = true)\n",
      " |-- Soil_type_7757: integer (nullable = true)\n",
      " |-- Soil_type_7790: integer (nullable = true)\n",
      " |-- Soil_type_8703: integer (nullable = true)\n",
      " |-- Soil_type_8707: integer (nullable = true)\n",
      " |-- Soil_type_8708: integer (nullable = true)\n",
      " |-- Soil_type_8771: integer (nullable = true)\n",
      " |-- Soil_type_8772: integer (nullable = true)\n",
      " |-- Soil_type_8776: integer (nullable = true)\n",
      " |-- CoverType: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "#Creazione modello di ML\n",
    "\n",
    "#Cerchiamo di predirre la colonna Elevation che è la prima nel dataset\n",
    "\n",
    "#Usiamo una pipeline in 2 stadi per la creazione di modelli ML in PySpark\n",
    "\n",
    "#1) Istanzione un oggetto della classe VectorAssembler\n",
    "#Che per permette di fondere tutte le colonne in una\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[1:] #la 1 è \"elevation\"\n",
    "    , outputCol='features'\n",
    "    )\n",
    "\n",
    "#2) Istanzione un oggetto della classe RandomForestRegressor\n",
    "#Implementa l'algoritmo di regressione \"Random Forest\" e crea il modello ML\n",
    "rf_obj = rg.RandomForestRegressor(\n",
    "    labelCol='Elevation' #target value (colonna da predire)\n",
    "    , maxDepth=10\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    "    , numTrees=10\n",
    "    )\n",
    "\n",
    "#Pipeline \n",
    "pip = Pipeline(stages=[vectorAssembler, rf_obj])\n",
    "\n",
    "#Modello ML \n",
    "pModel = ( #DF come quello di input ma con le colonne features e predicition \n",
    "    pip.fit(forest_train)\n",
    ")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "save() missing 1 required positional argument: 'path'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_303971/3623122374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mpModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "#Esportazione dati\n",
    "\n",
    "#Salvo il modello ML ottenuto\n",
    "modelPath=\"/home/lorenzo/Documenti/PySpark-tesi/Prototipo/exported\"\n",
    "pModel.write().overwrite().save(modelPath)\n",
    "\n",
    "#Esporto lo schema del dataset forest\n",
    "\n",
    "with open(modelPath+\"/schema.json\", \"w\") as f:\n",
    "    json.dump(forest.schema.jsonValue(), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#Test del modello su forest_test (solo batch)\n",
    "results=(\n",
    "    pModel\n",
    "    .transform(forest_test)\n",
    "    .select(\"Elevation\",\"prediction\")\n",
    ")\n",
    "\n",
    "#5 predizioni di esempio (da forest_train)\n",
    "results.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+------------------+\n",
      "|Elevation|        prediction|\n",
      "+---------+------------------+\n",
      "|     1879|2133.8356236533614|\n",
      "|     1896|2164.4272321820235|\n",
      "|     1901| 2092.579629892129|\n",
      "|     1904|1977.2831955407844|\n",
      "|     1905|1976.4692019172376|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#Valutazione accuratezza \n",
    "evaluator = ev.RegressionEvaluator(labelCol='Elevation')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'r2'})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.821924491416199"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "#Utilizzo il modello cercando di predirre \"elavation\" nei record ricevuti dalla socket\n",
    "\n",
    "#Disattivo il log di livello ALL (altrimenti output illegibile)\n",
    "spark.sparkContext.setLogLevel('error')\n",
    "\n",
    "#DF che rappresenta i dati letti dalla socket\n",
    "socketDF = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', 'localhost')\\\n",
    "    .option('port', 9999)\\\n",
    "    \n",
    "    \n",
    "\n",
    "#Esporto lo schema del DF di partenza: forest\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}