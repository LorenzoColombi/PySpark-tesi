{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/07/09 21:08:37 WARN Utils: Your hostname, lorenzo-X555UJ resolves to a loopback address: 127.0.1.1; using 192.168.1.141 instead (on interface wlp3s0)\n",
      "21/07/09 21:08:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/07/09 21:08:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/07/09 21:08:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#INIZIALIZZAIZIONE SESSION E AVVIO SPARK\n",
    "import pyspark as pys \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql as sql\n",
    "import pyspark.sql.functions as f\n",
    "# #Spark Config (chiamare una volta sola)\n",
    "conf = SparkConf().setAppName(\"learning RDD\")\n",
    "#sc = SparkContext(conf=conf) \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "\n",
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- ScreenSize: string (nullable = true)\n",
      " |-- RAM: string (nullable = true)\n",
      " |-- HDD: string (nullable = true)\n",
      " |-- W: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creazione DataFrame\n",
    "#creo RDD\n",
    "#sc = oggetto context\n",
    "sample_data = sc.parallelize([\n",
    "(1, 'MacBook Pro', 2015, '15\"', '16GB', '512GB SSD'\n",
    ", 13.75, 9.48, 0.61, 4.02)\n",
    ", (2, 'MacBook', 2016, '12\"', '8GB', '256GB SSD'\n",
    ", 11.04, 7.74, 0.52, 2.03)\n",
    ", (3, 'MacBook Air', 2016, '13.3\"', '8GB', '128GB SSD'\n",
    ", 12.8, 8.94, 0.68, 2.96)\n",
    ", (4, 'iMac', 2017, '27\"', '64GB', '1TB SSD'\n",
    ", 25.6, 8.0, 20.3, 20.8)\n",
    "])\n",
    "\n",
    "#Converto in DF\n",
    "#spark = oggetto sessione \n",
    "sample_data_df = spark.createDataFrame(sample_data,[\n",
    "    'Id'\n",
    ", 'Model'\n",
    ", 'Year'\n",
    ", 'ScreenSize'\n",
    ", 'RAM'\n",
    ", 'HDD'\n",
    ", 'W'\n",
    ", 'D'\n",
    ", 'H'\n",
    ", 'Weight'\n",
    "])\n",
    "\n",
    "sample_data_df.take(1) #prima riga\n",
    "sample_data_df.show() #tutto il DF\n",
    "sample_data_df.printSchema() #schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+--------------------+\n",
      "| id|                 val|\n",
      "+---+--------------------+\n",
      "|  0|  0.9150914763864632|\n",
      "|  1|  0.9204197144347012|\n",
      "|  2|0.016164751339321604|\n",
      "+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+--------------------+-------------------+\n",
      "| id|                 val|        probability|\n",
      "+---+--------------------+-------------------+\n",
      "|  0|  0.9150914763864632|0.26246573513694355|\n",
      "|  1|  0.9204197144347012| 0.2611854053393164|\n",
      "|  2|0.016164751339321604|0.39889016215958295|\n",
      "|  3| 0.37891472354166345| 0.3713067579948149|\n",
      "|  4|  0.8465682773911519|  0.278795300237918|\n",
      "+---+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vectorized UDF\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "big_df = (\n",
    "spark\n",
    ".range(0, 1000000)\n",
    ".withColumn('val', f.rand())\n",
    ")\n",
    "big_df.cache()\n",
    "big_df.show(3)\n",
    "#UDF con decoratore (deprecato in PySpark 3. Usare hints)\n",
    "@f.pandas_udf('double')\n",
    "def pandas_pdf(v):\n",
    "    return pd.Series(stats.norm.pdf(v))\n",
    "(\n",
    "big_df\n",
    ".withColumn('probability', pandas_pdf(big_df.val))\n",
    ".show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n+---+-----------+----+----------+----+---------+-----+----+----+------+\n|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n+---+-----------+----+----------+----+---------+-----+----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Manca esempio su introspezione NB\n",
    "\n",
    "#Definire uno schema\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "#Definisco lo schema\n",
    "sch = typ.StructType([\n",
    "    typ.StructField('Id', typ.LongType(), False)\n",
    "    , typ.StructField('Model', typ.StringType(), True)\n",
    "    , typ.StructField('Year', typ.IntegerType(), True)\n",
    "    , typ.StructField('ScreenSize', typ.StringType(), True)\n",
    "    , typ.StructField('RAM', typ.StringType(), True)\n",
    "    , typ.StructField('HDD', typ.StringType(), True)\n",
    "    , typ.StructField('W', typ.DoubleType(), True)\n",
    "    , typ.StructField('D', typ.DoubleType(), True)\n",
    "    , typ.StructField('H', typ.DoubleType(), True)\n",
    "    , typ.StructField('Weight', typ.DoubleType(), True)\n",
    "])\n",
    "\n",
    "sample_data_rdd = sample_data #definito prima\n",
    "sample_data_rdd.take(5)\n",
    "\n",
    "header = sample_data_rdd.first()\n",
    "\n",
    "sample_data_rdd = (\n",
    "    sample_data_rdd\n",
    "    .filter(lambda row: row != header)\n",
    "    .map(lambda row: (\n",
    "        int(row[0])\n",
    "        , row[1]\n",
    "        , int(row[2])\n",
    "        , row[3]\n",
    "        , row[4]\n",
    "        , row[5]\n",
    "        , float(row[6])\n",
    "        , float(row[7])\n",
    "        , float(row[8])\n",
    "        , float(row[9])\n",
    "          )\n",
    "        )\n",
    "    )\n",
    "sample_data_schema = spark.createDataFrame(sample_data_rdd, schema=sch)\n",
    "sample_data_schema.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISTE TEMPORANEA\n",
    "#sample_data_schema è richiesto\n",
    "sample_data_schema.createTempView('sample_data_view')\n",
    "#Meglio usare .createOrReplaceTempView(nome)\n",
    "\n",
    "#ATTENZIONE non può creare più di una volta la stessa vista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+----+\n",
      "|      Model|Year|\n",
      "+-----------+----+\n",
      "|    MacBook|2016|\n",
      "|MacBook Air|2016|\n",
      "+-----------+----+\n",
      "\n",
      "+-----------+----------+\n",
      "|      Model|FormFactor|\n",
      "+-----------+----------+\n",
      "|MacBook Pro|    Laptop|\n",
      "|    MacBook|    Laptop|\n",
      "|MacBook Air|    Laptop|\n",
      "|       iMac|   Desktop|\n",
      "+-----------+----------+\n",
      "\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|FormFactor|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|   Desktop|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|    Laptop|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|    Laptop|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL sulla vista\n",
    "sample_data_schema.createOrReplaceTempView('sample_data_view')\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT Model, Year\n",
    "    FROM sample_data_view\n",
    "    WHERE Weight<10\n",
    "''').show()\n",
    "\n",
    "#Creo un nuovo DF da un RDD\n",
    "models_df = sc.parallelize([\n",
    "('MacBook Pro', 'Laptop')\n",
    ", ('MacBook', 'Laptop')\n",
    ", ('MacBook Air', 'Laptop')\n",
    ", ('iMac', 'Desktop')\n",
    "]).toDF(['Model','FormFactor'])\n",
    "\n",
    "models_df.show()\n",
    "\n",
    "#Faccio il JOIN\n",
    "models_df.createOrReplaceTempView('models')\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT S.*,M.FormFactor\n",
    "    FROM sample_data_view AS S\n",
    "        JOIN models as M ON S.Model = M.Model\n",
    "    ORDER BY Weight DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight| HDDsplitted|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|[256GB, SSD]|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|[128GB, SSD]|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|  [1TB, SSD]|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+------------+\n",
      "\n",
      "+-------+------------------+-----------------+\n",
      "|summary|                 D|                W|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|                 3|                3|\n",
      "|   mean| 8.226666666666667|            16.48|\n",
      "| stddev|0.6312949653952049|7.947024600440093|\n",
      "|    min|              7.74|            11.04|\n",
      "|    25%|              7.74|            11.04|\n",
      "|    50%|               8.0|             12.8|\n",
      "|    75%|              8.94|             25.6|\n",
      "|    max|              8.94|             25.6|\n",
      "+-------+------------------+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|RAM_freqItems|\n",
      "+-------------+\n",
      "|  [64GB, 8GB]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Alcune trasformazioni\n",
    "\n",
    "#filter\n",
    "sample_data_schema.filter(sample_data_schema.Year>2015).show()\n",
    "\n",
    "#withColumn\n",
    "sample_data_schema.withColumn(\"HDDsplitted\",f.split(f.col(\"HDD\"),\" \")).show()\n",
    "\n",
    "#summary\n",
    "sample_data_schema.select(\"D\",\"W\").summary().show()\n",
    "\n",
    "#freqitems\n",
    "sample_data_schema.freqItems(['RAM']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Id        Model  Year ScreenSize   RAM        HDD      W     D      H  \\\n",
       "0   2      MacBook  2016        12\"   8GB  256GB SSD  11.04  7.74   0.52   \n",
       "1   3  MacBook Air  2016      13.3\"   8GB  128GB SSD  12.80  8.94   0.68   \n",
       "2   4         iMac  2017        27\"  64GB    1TB SSD  25.60  8.00  20.30   \n",
       "\n",
       "   Weight  \n",
       "0    2.03  \n",
       "1    2.96  \n",
       "2   20.80  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Model</th>\n      <th>Year</th>\n      <th>ScreenSize</th>\n      <th>RAM</th>\n      <th>HDD</th>\n      <th>W</th>\n      <th>D</th>\n      <th>H</th>\n      <th>Weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>MacBook</td>\n      <td>2016</td>\n      <td>12\"</td>\n      <td>8GB</td>\n      <td>256GB SSD</td>\n      <td>11.04</td>\n      <td>7.74</td>\n      <td>0.52</td>\n      <td>2.03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>MacBook Air</td>\n      <td>2016</td>\n      <td>13.3\"</td>\n      <td>8GB</td>\n      <td>128GB SSD</td>\n      <td>12.80</td>\n      <td>8.94</td>\n      <td>0.68</td>\n      <td>2.96</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>iMac</td>\n      <td>2017</td>\n      <td>27\"</td>\n      <td>64GB</td>\n      <td>1TB SSD</td>\n      <td>25.60</td>\n      <td>8.00</td>\n      <td>20.30</td>\n      <td>20.80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "#Alcune Azioni\n",
    "\n",
    "#toPandas\n",
    "sample_data_schema.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}