{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#INIZIALIZZAIZIONE SESSION E AVVIO SPARK\n",
    "import pyspark as pys \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql as sql\n",
    "\n",
    "#Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Spark context\n",
    "sc = spark.sparkContext\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#CARICAMENTO DF\n",
    "\n",
    "forest_path = './learningPySpark/Data/forest_coverage_type.csv'\n",
    "\n",
    "forest=spark.read.csv(\n",
    "    forest_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "forest.printSchema()\n",
    "#Sono tutti campi numerici"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Transformers\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.ml.feature as feat\n",
    "import numpy as np\n",
    "\n",
    "#Bucketizer\n",
    "bucket_no=10 \n",
    "\n",
    "dist_min_max = (\n",
    "    forest.agg( #prendo min e max\n",
    "        f.min('Horizontal_Distance_To_Hydrology').alias(\"min\")\n",
    "        ,f.max('Horizontal_Distance_To_Hydrology').alias(\"max\")\n",
    "    ).rdd\n",
    "    .map(lambda row: (row.min,row.max))\n",
    "    .collect()[0] #Non prenso la lista ma solo il primo (unico) elemento\n",
    "\n",
    ")\n",
    "#range\n",
    "rng = dist_min_max[1]-dist_min_max[0]\n",
    "\n",
    "#Prendo 11 punti equidistanti nel range\n",
    "#Valori limite di ogni bucket (11 tagli = 10 bucket)\n",
    "#Nei tagli contano anche gli estrmi\n",
    "splits = list(np.arange(\n",
    "    dist_min_max[0],\n",
    "    dist_min_max[1],\n",
    "    rng/ (bucket_no+1)\n",
    ")\n",
    ")\n",
    "\n",
    "#Creazione oggetto\n",
    "bucketizer = feat.Bucketizer(\n",
    "    splits=splits, #array con valori in cui tagliare\n",
    "    inputCol=\"Horizontal_Distance_To_Hydrology\",\n",
    "    outputCol=\"Horizontal_Distance_To_Hydrology_Bkt\"\n",
    ")\n",
    "\n",
    "#Trasformazione\n",
    "(\n",
    "    bucketizer.transform(forest).select(\n",
    "        'Horizontal_Distance_To_Hydrology'\n",
    "        ,'Horizontal_Distance_To_Hydrology_Bkt'\n",
    "    ).show(5)\n",
    ")\n",
    "\n",
    "#DF con 2 colonne, il valori continuo e il numero di bucket"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Gli estimators (vedi sotto) vogliono una sola colonna\n",
    "#Effettuiamo in raggrupamento\n",
    "\n",
    "#raggruppiamo tutte e colonne nella colonna feat\n",
    "vectorAssembler = (\n",
    "    feat.VectorAssembler(inputCols=forest.columns,outputCol='feat')\n",
    ")\n",
    "\n",
    "#Prendiamo le 5 variabili (feature) più significative\n",
    "pca = (\n",
    "    feat.PCA(k=5,inputCol=vectorAssembler.getOutputCol(),outputCol='pca_feat')\n",
    ")\n",
    "\n",
    "(\n",
    "    pca\n",
    "    .fit(vectorAssembler.transform(forest))\n",
    "    .transform(vectorAssembler.transform(forest))\n",
    "    .select(\"feat\",\"pca_feat\")\n",
    "    .take(1)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#ESTIMATORS\n",
    "import pyspark.ml.classification as cl\n",
    "\n",
    "#Cerchiamo di predirre quando la foresta avrà \"cover type\" = 1 (spruce-fir conifere)\n",
    "\n",
    "#Creiamo un SVM MODEL\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[0:-1] #selezioniamo tutte le colonne tranne l'ultima (cover type)\n",
    "    , outputCol=\"features\"\n",
    ")\n",
    "\n",
    "fir_dataset = (\n",
    "    vectorAssembler.transform(forest).withColumn( #aggiunge colonne\n",
    "        # (NomeColonna,Dati)\n",
    "        \"label\"\n",
    "        ,(f.col(\"CoverType\")==1).cast(\"integer\") #1 = conifera\n",
    "    )\n",
    "    .select(\"label\",\"features\")\n",
    ")\n",
    "\n",
    "svc_obj = cl.LinearSVC(maxIter=10,regParam=0.01)\n",
    "svc_model = svc_obj.fit(fir_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Estraiamo il risultato della modellizazione\n",
    "svc_model.coefficients"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Regressione lineare\n",
    "import pyspark.ml.regression as rg\n",
    "\n",
    "#Cerchiamo di stimare l'altezza di una foresta\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[1:]\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "\n",
    "elevation_dataset = (\n",
    "    vectorAssembler.transform(forest).withColumn( #aggiunge colonne\n",
    "        \"label\",\n",
    "        f.col(\"Elevation\").cast(\"float\")#regressione con variabili continue\n",
    "    )\n",
    "    .select(\"label\",\"features\")\n",
    ")\n",
    "\n",
    "#Modello regressione lineare\n",
    "lr_obj = rg.LinearRegression(maxIter=10,regParam=0.01,elasticNetParam=1.00)\n",
    "lr_model=lr_obj.fit(elevation_dataset)\n",
    "\n",
    "#Il codice per creare un modello è abbastanza standard è di facile realizzazione"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Risultati della modellazione\n",
    "lr_model.coefficients\n",
    "\n",
    "#Resoconto automatico\n",
    "summary = lr_model.summary\n",
    "\n",
    "print(\n",
    "    summary.r2 #78% buon risultato\n",
    "    , summary.rootMeanSquaredError\n",
    "    ,summary.meanAbsoluteError\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#PIPELINES\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#link function\n",
    "vectorAssembler= feat.VectorAssembler(\n",
    "    inputCols=forest.columns[1:],outputCol=\"features\"\n",
    ")\n",
    "\n",
    "#model\n",
    "lr_obj=rg.GeneralizedLinearRegression(\n",
    "    labelCol=\"Elevation\"\n",
    "    ,maxIter=10\n",
    "    ,regParam=0.01\n",
    "    ,link=\"identity\"\n",
    "    ,linkPredictionCol=\"p\"\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler,lr_obj])\n",
    "\n",
    "(\n",
    "    pip\n",
    "    .fit(forest)\n",
    "    .transform(forest)\n",
    "    .select(\"Elevation\",\"prediction\")\n",
    "    .show(5)\n",
    ")\n",
    "\n",
    "#Il modello predittivo è abbastanza accurato\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Istogramma di elevation per capire quale modello usare (usato sopra)\n",
    "import matplotlib.pyplot as plt\n",
    "transformed_df = forest.select('Elevation')\n",
    "transformed_df.toPandas().hist()\n",
    "plt.savefig('Elevation_histogram.png')\n",
    "plt.close('all')\n",
    "\n",
    "#Sembra una ditribuzione normale"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#SELEZIONARE LE VARIABILI PIÙ PREDITTIBILI\n",
    "\n",
    "#TOP 10 variabili per predirre la classe si un'osservazione\n",
    "\n",
    "#Raggruppiamo tutte le colonne in una\n",
    "#Tranne l'ultima che è coverType ovvero il nostro obiettivo\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[0:-1]\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "\n",
    "#Selezioni migliore feature usando il chi-square test\n",
    "selector = feat.ChiSqSelector(\n",
    "    labelCol=\"CoverType\" #colonna obiettivo\n",
    "    ,numTopFeatures=10\n",
    "    ,outputCol=\"selected\" #Vettore con le prime 10 colonne\n",
    ")\n",
    "\n",
    "pipeline_sel = Pipeline(stages=[vectorAssembler,selector])\n",
    "\n",
    "(\n",
    "    pipeline_sel\n",
    "    .fit(forest)\n",
    "    .transform(forest)\n",
    "    .select(selector.getOutputCol())\n",
    "    .show(5)\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Selezione di variabili continue, usando la correlazione\n",
    "import pyspark.ml.stat as st\n",
    "\n",
    "features_and_label = feat.VectorAssembler(\n",
    "    inputCols=forest.columns\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "\n",
    "#Correlazione\n",
    "corr = st.Correlation.corr(\n",
    "    features_and_label.transform(forest)\n",
    "    ,\"features\"\n",
    "    ,\"pearson\" #Coeff di Pearson\n",
    ")\n",
    "\n",
    "print(str(corr.collect()[0][0]))\n",
    "\n",
    "#Estraiamo le 10 colonne più correlate col il nostro label (covertype)\n",
    "\n",
    "num_of_features = 10\n",
    "cols =dict([\n",
    "    (i,e) for i, e in enumerate (forest.columns)\n",
    "])\n",
    "\n",
    "corr_matrix = corr.collect()[0][0]\n",
    "label_corr_idx = [\n",
    "    (i[0],e) for i,e in np.ndenumerate(corr_matrix.toArray()[:,0])\n",
    "][1:]\n",
    "\n",
    "label_corr_idx_sorted = sorted(\n",
    "    label_corr_idx\n",
    "    , key=lambda el: -abs(el[1])\n",
    ")\n",
    "\n",
    "features_selected = np.array([\n",
    "    cols[el[0]]\n",
    "    for el in label_corr_idx_sorted\n",
    "])[0:num_of_features]\n",
    "features_selected\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Predire forest coverage type\n",
    "#Usando il logistic regression model\n",
    "\n",
    "#Divisione fra train e test\n",
    "forest_train, forest_test=(\n",
    "    forest.randomSplit([0.7,0.3],seed=666)\n",
    ")\n",
    "#Uniamo le colonne in una\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[0:-1]\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "#Selezioniamo le 10 feature più utili\n",
    "selector = feat.ChiSqSelector(\n",
    "    labelCol=\"CoverType\"\n",
    "    ,numTopFeatures=10\n",
    "    ,outputCol=\"selected\"\n",
    ")\n",
    "\n",
    "#Logistic regression model\n",
    "logReg_obj = cl.LogisticRegression(\n",
    "    labelCol=\"CoverType\"\n",
    "    ,featuresCol=selector.getOutputCol()\n",
    "    ,regParam=0.01\n",
    "    ,elasticNetParam=1.0\n",
    "    ,family=\"multinomial\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[vectorAssembler,selector,logReg_obj]\n",
    ")\n",
    "\n",
    "pModel=pipeline.fit(forest_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Testiamo il modello\n",
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "results_logReg=(\n",
    "    pModel\n",
    "    .transform(forest_test)\n",
    "    .select(\"CoverType\",\"probability\",\"prediction\")\n",
    ")\n",
    "\n",
    "evaluator = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\"\n",
    "    ,labelCol=\"CoverType\"\n",
    ")\n",
    "\n",
    "\n",
    "(\n",
    "    evaluator.evaluate(results_logReg)\n",
    "    , evaluator.evaluate(results_logReg\n",
    "    , {evaluator.metricName: 'weightedPrecision'}\n",
    ")\n",
    "    , evaluator.evaluate(\n",
    "    results_logReg\n",
    "    , {evaluator.metricName: 'accuracy'}\n",
    ")\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Stima forest elevation\n",
    "\n",
    "#XXX\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[1:] #la 1 è l'obiettivo\n",
    "    , outputCol='features'\n",
    "    )\n",
    "\n",
    "rf_obj = rg.RandomForestRegressor(\n",
    "    labelCol='Elevation'\n",
    "    , maxDepth=10\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    "    , numTrees=10\n",
    "    )\n",
    "pip = Pipeline(stages=[vectorAssembler, rf_obj])\n",
    "\n",
    "#Test performance\n",
    "results = (\n",
    "pip\n",
    "    .fit(forest)\n",
    "    .transform(forest)\n",
    "    .select('Elevation', 'prediction')\n",
    ")\n",
    "evaluator = ev.RegressionEvaluator(labelCol='Elevation')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'r2'})\n",
    "\n",
    "#83% meglio della regressione lineare"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#MODELLO DI CLUSTERING\n",
    "import pyspark.ml.clustering as clust\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[:-1]\n",
    "    , outputCol='features'\n",
    ")\n",
    "\n",
    "kmeans_obj = clust.KMeans(k=7,seed=666) #k numero di cluster previsto\n",
    "\n",
    "pip=Pipeline(stages=[vectorAssembler,kmeans_obj])\n",
    "\n",
    "#Guardiamo i risultati\n",
    "results = (\n",
    "    pip\n",
    "    .fit(forest)\n",
    "    .transform(forest)\n",
    "    .select('features', 'CoverType', 'prediction')\n",
    ")\n",
    "results.show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Valutazione risultati\n",
    "clustering_ev = ev.ClusteringEvaluator()\n",
    "clustering_ev.evaluate(results)\n",
    "\n",
    "#0.50 buon risultato"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Tuning dei parametri\n",
    "#Meglio non eseguire questa cella, esecuzione molto lunga\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "inputCols=forest.columns[0:-1]\n",
    ", outputCol='features')\n",
    "\n",
    "selector = feat.ChiSqSelector(\n",
    "labelCol='CoverType'\n",
    ", numTopFeatures=5\n",
    ", outputCol='selected')\n",
    "\n",
    "logReg_obj = cl.LogisticRegression(\n",
    "labelCol='CoverType'\n",
    ", featuresCol=selector.getOutputCol()\n",
    ", family='multinomial'\n",
    ")\n",
    "#Tuning di 2 parametri con 2 livelli ognuno -> 4 modelli da creare\n",
    "logReg_grid = (\n",
    "    tune.ParamGridBuilder()\n",
    "    .addGrid(logReg_obj.regParam\n",
    "    , [0.01, 0.1]\n",
    "    )\n",
    "    .addGrid(logReg_obj.elasticNetParam\n",
    "    , [1.0, 0.5]\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "logReg_ev = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction'\n",
    "    , labelCol='CoverType')\n",
    "\n",
    "cross_v = tune.CrossValidator(\n",
    "    estimator=logReg_obj\n",
    "    , estimatorParamMaps=logReg_grid\n",
    "    , evaluator=logReg_ev\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, selector])\n",
    "data_trans = pipeline.fit(forest_train)\n",
    "\n",
    "logReg_modelTest = cross_v.fit(\n",
    "data_trans.transform(forest_train)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Testiamo il modello\n",
    "data_trans_test = data_trans.transform(forest_test)\n",
    "results = logReg_modelTest.transform(data_trans_test)\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedPrecision'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedRecall'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'accuracy'}))\n",
    "\n",
    "#È peggiorata a causa della scelta di 5 feature e non 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Estrarre feauture dal testo\n",
    "import pyspark.ml.feature as feat\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Si splitta sugli spazi e infine ci contano le ricorrenze di ogni parola e attraverso un hashing portarci il variabili numeriche\n",
    "\n",
    "#Definisco dataset di frasi\n",
    "\n",
    "some_text = spark.createDataFrame([\n",
    "['''\n",
    "Apache Spark achieves high performance for both batch\n",
    "and streaming data, using a state-of-the-art DAG scheduler,\n",
    "a query optimizer, and a physical execution engine.\n",
    "''']\n",
    ", ['''\n",
    "Apache Spark is a fast and general-purpose cluster computing\n",
    "system. It provides high-level APIs in Java, Scala, Python\n",
    "and R, and an optimized engine that supports general execution\n",
    "graphs. It also supports a rich set of higher-level tools including\n",
    "Spark SQL for SQL and structured data processing, MLlib for machine\n",
    "learning, GraphX for graph processing, and Spark Streaming.\n",
    "''']\n",
    ", ['''\n",
    "Machine learning is a field of computer science that often uses\n",
    "statistical techniques to give computers the ability to \"learn\"\n",
    "(i.e., progressively improve performance on a specific task)\n",
    "with data, without being explicitly programmed.\n",
    "''']\n",
    "], ['text'])\n",
    "\n",
    "\n",
    "#Splitto usando spazi , . \\ \"\n",
    "splitter = feat.RegexTokenizer(\n",
    "    inputCol='text'\n",
    "    , outputCol='text_split'\n",
    "    , pattern='\\s+|[,.\\\"]'\n",
    ")\n",
    "\n",
    "#Rimuovo parole non significative (stop-word)\n",
    "sw_remover = feat.StopWordsRemover(\n",
    "    inputCol=splitter.getOutputCol()\n",
    "    , outputCol='no_stopWords'\n",
    ")\n",
    "\n",
    "#Hashing\n",
    "hasher = feat.HashingTF(\n",
    "    inputCol=sw_remover.getOutputCol()\n",
    "    , outputCol='hashed'\n",
    "    , numFeatures=20\n",
    ")\n",
    "\n",
    "#frequenty-inverse document frequency\n",
    "#frequenza della parola nel testo / in tutti i testi\n",
    "#Misura l'importanza della parola nel testo\n",
    "idf = feat.IDF(\n",
    "    inputCol=hasher.getOutputCol()\n",
    "    , outputCol='features'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[splitter, sw_remover, hasher, idf])\n",
    "pipelineModel = pipeline.fit(some_text)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Disretizzazione variabili continue\n",
    "\n",
    "#Usiamo un DF diverso da prima\n",
    "signal_df = spark.read.csv(\n",
    "'./learningPySpark/Data/fourier_signal.csv'\n",
    ", header=True\n",
    ", inferSchema=True\n",
    ")\n",
    "\n",
    "#Discretizzatore\n",
    "steps = feat.QuantileDiscretizer(\n",
    "    numBuckets=10,\n",
    "    inputCol='signal',\n",
    "    outputCol='discretized')\n",
    "\n",
    "#DF trasformato\n",
    "transformed = (\n",
    "    steps\n",
    "    .fit(signal_df)\n",
    "    .transform(signal_df)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Standardizzazione variabili continue\n",
    "\n",
    "#Rappresentazione vettorizzata\n",
    "vec = feat.VectorAssembler(\n",
    "    inputCols=['signal']\n",
    "    , outputCol='signal_vec'\n",
    ")\n",
    "\n",
    "norm = feat.StandardScaler( #accetta solo Rappresentazione vettorizzata\n",
    "    inputCol=vec.getOutputCol()\n",
    "    , outputCol='signal_norm'\n",
    "    , withMean=True #=0\n",
    "    , withStd=True #=1\n",
    ")\n",
    "\n",
    "norm_pipeline = Pipeline(stages=[vec, norm])\n",
    "signal_norm = (\n",
    "    norm_pipeline\n",
    "    .fit(signal_df)\n",
    "    .transform(signal_df)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Divisione in bucket in base alle parole\n",
    "\n",
    "articles = spark.createDataFrame([\n",
    "('''\n",
    "    The Andromeda Galaxy, named after the mythological\n",
    "    Princess Andromeda, also known as Messier 31, M31,\n",
    "    or NGC 224, is a spiral galaxy approximately 780\n",
    "    kiloparsecs (2.5 million light-years) from Earth,\n",
    "    and the nearest major galaxy to the Milky Way.\n",
    "    Its name stems from the area of the sky in which it\n",
    "    appears, the constellation of Andromeda. The 2006\n",
    "    observations by the Spitzer Space Telescope revealed\n",
    "    that the Andromeda Galaxy contains approximately one\n",
    "    trillion stars, more than twice the number of the\n",
    "    Milky Way’s estimated 200-400 billion stars. The\n",
    "    Andromeda Galaxy, spanning approximately 220,000 light\n",
    "    years, is the largest galaxy in our Local Group,\n",
    "    which is also home to the Triangulum Galaxy and\n",
    "    other minor galaxies. The Andromeda Galaxy's mass is\n",
    "    estimated to be around 1.76 times that of the Milky\n",
    "    Way Galaxy (~0.8-1.5×1012 solar masses vs the Milky\n",
    "    Way's 8.5×1011 solar masses).\n",
    "    ''','Galaxy', 'Andromeda')\n",
    "    (...)\n",
    ", ('''\n",
    "Washington, officially the State of Washington, is a state in the Pacific\n",
    "Northwest region of the United States. Named after George Washington,\n",
    "the first president of the United States, the state was made out of the\n",
    "western part of the Washington Territory, which was ceded by Britain in\n",
    "1846 in accordance with the Oregon Treaty in the settlement of the\n",
    "Oregon boundary dispute. It was admitted to the Union as the 42nd state\n",
    "in 1889. Olympia is the state capital. Washington is sometimes referred\n",
    "to as Washington State, to distinguish it from Washington, D.C., the\n",
    "capital of the United States, which is often shortened to Washington.\n",
    "''','Geography', 'Washington State')\n",
    "], ['articles', 'Topic', 'Object'])\n",
    "\n",
    "#Come visto prima\n",
    "\n",
    "splitter = feat.RegexTokenizer(\n",
    "inputCol='articles'\n",
    ", outputCol='articles_split'\n",
    ", pattern='\\s+|[,.\\\"]'\n",
    ")\n",
    "sw_remover = feat.StopWordsRemover(\n",
    "inputCol=splitter.getOutputCol()\n",
    ", outputCol='no_stopWords'\n",
    ")\n",
    "count_vec = feat.CountVectorizer(inputCol=sw_remover.getOutputCol()\n",
    ", outputCol='vector'\n",
    ")\n",
    "\n",
    "#Clusterizzazione\n",
    "#Latent dirichlet allocation\n",
    "lda_clusters = clust.LDA(\n",
    "    k=3 #ci aspettimo 3 cluster\n",
    "    , optimizer='online'\n",
    "    , featuresCol=count_vec.getOutputCol()\n",
    ")\n",
    "\n",
    "topic_pipeline = Pipeline(\n",
    "    stages=[\n",
    "    splitter\n",
    "    , sw_remover\n",
    "    , count_vec\n",
    "    , lda_clusters\n",
    "    ]\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Test\n",
    "for topic in (\n",
    "    topic_pipeline\n",
    "    .fit(articles)\n",
    "    .transform(articles)\n",
    "    .select('Topic','Object','topicDistribution')\n",
    "    .take(10)\n",
    "):\n",
    "    print(\n",
    "    topic.Topic\n",
    "    , topic.Object\n",
    "    , np.argmax(topic.topicDistribution)\n",
    "    , topic.topicDistribution\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%help"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Cell magic `%%help` not found.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}