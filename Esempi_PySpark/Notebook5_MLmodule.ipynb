{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/07/16 14:51:03 WARN Utils: Your hostname, lorenzo-X555UJ resolves to a loopback address: 127.0.1.1; using 192.168.43.245 instead (on interface wlp3s0)\n",
      "21/07/16 14:51:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/07/16 14:51:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#INIZIALIZZAIZIONE SESSION E AVVIO SPARK\n",
    "import pyspark as pys \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql as sql\n",
    "\n",
    "#Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Spark context\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_Rawah: integer (nullable = true)\n",
      " |-- Wilderness_Area_Neota: integer (nullable = true)\n",
      " |-- Wilderness_Area_Comanche: integer (nullable = true)\n",
      " |-- Wilderness_Area_CacheLaPoudre: integer (nullable = true)\n",
      " |-- Soil_type_2702: integer (nullable = true)\n",
      " |-- Soil_type_2703: integer (nullable = true)\n",
      " |-- Soil_type_2704: integer (nullable = true)\n",
      " |-- Soil_type_2705: integer (nullable = true)\n",
      " |-- Soil_type_2706: integer (nullable = true)\n",
      " |-- Soil_type_2717: integer (nullable = true)\n",
      " |-- Soil_type_3501: integer (nullable = true)\n",
      " |-- Soil_type_3502: integer (nullable = true)\n",
      " |-- Soil_type_4201: integer (nullable = true)\n",
      " |-- Soil_type_4703: integer (nullable = true)\n",
      " |-- Soil_type_4704: integer (nullable = true)\n",
      " |-- Soil_type_4744: integer (nullable = true)\n",
      " |-- Soil_type_4758: integer (nullable = true)\n",
      " |-- Soil_type_5101: integer (nullable = true)\n",
      " |-- Soil_type_5151: integer (nullable = true)\n",
      " |-- Soil_type_6101: integer (nullable = true)\n",
      " |-- Soil_type_6102: integer (nullable = true)\n",
      " |-- Soil_type_6731: integer (nullable = true)\n",
      " |-- Soil_type_7101: integer (nullable = true)\n",
      " |-- Soil_type_7102: integer (nullable = true)\n",
      " |-- Soil_type_7103: integer (nullable = true)\n",
      " |-- Soil_type_7201: integer (nullable = true)\n",
      " |-- Soil_type_7202: integer (nullable = true)\n",
      " |-- Soil_type_7700: integer (nullable = true)\n",
      " |-- Soil_type_7701: integer (nullable = true)\n",
      " |-- Soil_type_7702: integer (nullable = true)\n",
      " |-- Soil_type_7709: integer (nullable = true)\n",
      " |-- Soil_type_7710: integer (nullable = true)\n",
      " |-- Soil_type_7745: integer (nullable = true)\n",
      " |-- Soil_type_7746: integer (nullable = true)\n",
      " |-- Soil_type_7755: integer (nullable = true)\n",
      " |-- Soil_type_7756: integer (nullable = true)\n",
      " |-- Soil_type_7757: integer (nullable = true)\n",
      " |-- Soil_type_7790: integer (nullable = true)\n",
      " |-- Soil_type_8703: integer (nullable = true)\n",
      " |-- Soil_type_8707: integer (nullable = true)\n",
      " |-- Soil_type_8708: integer (nullable = true)\n",
      " |-- Soil_type_8771: integer (nullable = true)\n",
      " |-- Soil_type_8772: integer (nullable = true)\n",
      " |-- Soil_type_8776: integer (nullable = true)\n",
      " |-- CoverType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CARICAMENTO DF\n",
    "\n",
    "forest_path = './learningPySpark/Data/forest_coverage_type.csv'\n",
    "\n",
    "forest=spark.read.csv(\n",
    "    forest_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "forest.printSchema()\n",
    "#Sono tutti campi numerici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "+--------------------------------+------------------------------------+\n",
      "|Horizontal_Distance_To_Hydrology|Horizontal_Distance_To_Hydrology_Bkt|\n",
      "+--------------------------------+------------------------------------+\n",
      "|                             258|                                 2.0|\n",
      "|                             212|                                 1.0|\n",
      "|                             268|                                 2.0|\n",
      "|                             242|                                 1.0|\n",
      "|                             153|                                 1.0|\n",
      "+--------------------------------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Transformers\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.ml.feature as feat\n",
    "import numpy as np\n",
    "\n",
    "#Bucketizer\n",
    "bucket_no=10 \n",
    "\n",
    "dist_min_max = (\n",
    "    forest.agg( #prendo min e max\n",
    "        f.min('Horizontal_Distance_To_Hydrology').alias(\"min\")\n",
    "        ,f.max('Horizontal_Distance_To_Hydrology').alias(\"max\")\n",
    "    ).rdd\n",
    "    .map(lambda row: (row.min,row.max))\n",
    "    .collect()[0] #Non prenso la lista ma solo il primo (unico) elemento\n",
    "\n",
    ")\n",
    "#range\n",
    "rng = dist_min_max[1]-dist_min_max[0]\n",
    "\n",
    "#Prendo 11 punti equidistanti nel range\n",
    "#Valori limite di ogni bucket (11 tagli = 10 bucket)\n",
    "#Nei tagli contano anche gli estrmi\n",
    "splits = list(np.arange(\n",
    "    dist_min_max[0],\n",
    "    dist_min_max[1],\n",
    "    rng/ (bucket_no+1)\n",
    ")\n",
    ")\n",
    "\n",
    "#Creazione oggetto\n",
    "bucketizer = feat.Bucketizer(\n",
    "    splits=splits, #array con valori in cui tagliare\n",
    "    inputCol=\"Horizontal_Distance_To_Hydrology\",\n",
    "    outputCol=\"Horizontal_Distance_To_Hydrology_Bkt\"\n",
    ")\n",
    "\n",
    "#Trasformazione\n",
    "(\n",
    "    bucketizer.transform(forest).select(\n",
    "        'Horizontal_Distance_To_Hydrology'\n",
    "        ,'Horizontal_Distance_To_Hydrology_Bkt'\n",
    "    ).show(5)\n",
    ")\n",
    "\n",
    "#DF con 2 colonne, il valori continuo e il numero di bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/07/14 21:00:48 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "21/07/14 21:00:48 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "21/07/14 21:00:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(feat=SparseVector(55, {0: 2596.0, 1: 51.0, 2: 3.0, 3: 258.0, 5: 510.0, 6: 221.0, 7: 232.0, 8: 148.0, 9: 6279.0, 10: 1.0, 42: 1.0, 54: 5.0}), pca_feat=DenseVector([-3887.7711, 4996.8103, 2323.0932, 1014.5873, -135.1702]))]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#Gli estimators (vedi sotto) vogliono una sola colonna\n",
    "#Effettuiamo in raggrupamento\n",
    "\n",
    "#raggruppiamo tutte e colonne nella colonna feat\n",
    "vectorAssembler = (\n",
    "    feat.VectorAssembler(inputCols=forest.columns,outputCol='feat')\n",
    ")\n",
    "\n",
    "#Prendiamo le 5 variabili (feature) più significative\n",
    "pca = (\n",
    "    feat.PCA(k=5,inputCol=vectorAssembler.getOutputCol(),outputCol='pca_feat')\n",
    ")\n",
    "\n",
    "(\n",
    "    pca\n",
    "    .fit(vectorAssembler.transform(forest))\n",
    "    .transform(vectorAssembler.transform(forest))\n",
    "    .select(\"feat\",\"pca_feat\")\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/07/14 21:12:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/07/14 21:12:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "#ESTIMATORS\n",
    "import pyspark.ml.classification as cl\n",
    "\n",
    "#Cerchiamo di predirre quando la foresta avrà \"cover type\" = 1 (spruce-fir conifere)\n",
    "\n",
    "#Creiamo un SVM MODEL\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[0:-1] #selezioniamo tutte le colonne tranne l'ultima (cover type)\n",
    "    , outputCol=\"features\"\n",
    ")\n",
    "\n",
    "fir_dataset = (\n",
    "    vectorAssembler.transform(forest).withColumn( #aggiunge colonne\n",
    "        # (NomeColonna,Dati)\n",
    "        \"label\"\n",
    "        ,(f.col(\"CoverType\")==1).cast(\"integer\") #1 = conifera\n",
    "    )\n",
    "    .select(\"label\",\"features\")\n",
    ")\n",
    "\n",
    "svc_obj = cl.LinearSVC(maxIter=10,regParam=0.01)\n",
    "svc_model = svc_obj.fit(fir_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DenseVector([-0.0001, -0.0, -0.0023, -0.0, -0.0001, 0.0, -0.001, -0.0017, -0.0003, -0.0, 0.0, 0.0401, -0.0071, -0.0958, -0.0901, -0.0653, -0.0655, -0.0437, -0.0928, -0.0848, -0.0211, -0.0045, -0.0498, -0.0829, -0.0522, -0.0325, -0.0263, -0.0923, -0.0889, -0.0275, -0.0606, -0.0595, 0.0341, -0.003, 0.0822, 0.0607, 0.0351, 0.0093, 0.0048, -0.0154, 0.0422, -0.0673, -0.0039, -0.0142, 0.0036, 0.0078, 0.0, -0.0117, 0.0283, -0.0002, -0.0463, 0.0394, 0.0292, 0.0358])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#Estraiamo il risultato della modellizazione\n",
    "svc_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "source": [
    "#Regressione lineare\n",
    "import pyspark.ml.regression as rg\n",
    "\n",
    "#Cerchiamo di stimare l'altezza di una foresta\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[1:]\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "\n",
    "elevation_dataset = (\n",
    "    vectorAssembler.transform(forest).withColumn( #aggiunge colonne\n",
    "        \"label\",\n",
    "        f.col(\"Elevation\").cast(\"float\")#regressione con variabili continue\n",
    "    )\n",
    "    .select(\"label\",\"features\")\n",
    ")\n",
    "\n",
    "#Modello regressione lineare\n",
    "lr_obj = rg.LinearRegression(maxIter=10,regParam=0.01,elasticNetParam=1.00)\n",
    "lr_model=lr_obj.fit(elevation_dataset)\n",
    "\n",
    "#Il codice per creare un modello è abbastanza standard è di facile realizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7860412464754158 129.50871925702575 103.34079732698777\n"
     ]
    }
   ],
   "source": [
    "#Risultati della modellazione\n",
    "lr_model.coefficients\n",
    "\n",
    "#Resoconto automatico\n",
    "summary = lr_model.summary\n",
    "\n",
    "print(\n",
    "    summary.r2 #78% buon risultato\n",
    "    , summary.rootMeanSquaredError\n",
    "    ,summary.meanAbsoluteError\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "+---------+------------------+\n",
      "|Elevation|        prediction|\n",
      "+---------+------------------+\n",
      "|     2596|2840.7801831411316|\n",
      "|     2590|2828.7464246669683|\n",
      "|     2804| 2842.761272955131|\n",
      "|     2785| 2966.057500325109|\n",
      "|     2595|2817.1687155114637|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PIPELINES\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#link function\n",
    "vectorAssembler= feat.VectorAssembler(\n",
    "    inputCols=forest.columns[1:],outputCol=\"features\"\n",
    ")\n",
    "\n",
    "#model\n",
    "lr_obj=rg.GeneralizedLinearRegression(\n",
    "    labelCol=\"Elevation\"\n",
    "    ,maxIter=10\n",
    "    ,regParam=0.01\n",
    "    ,link=\"identity\"\n",
    "    ,linkPredictionCol=\"p\"\n",
    ")\n",
    "\n",
    "pip = Pipeline(stages=[vectorAssembler,lr_obj])\n",
    "\n",
    "(\n",
    "    pip\n",
    "    .fit(forest)\n",
    "    .transform(forest)\n",
    "    .select(\"Elevation\",\"prediction\")\n",
    "    .show(5)\n",
    ")\n",
    "\n",
    "#Il modello predittivo è abbastanza accurato\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "source": [
    "#Istogramma di elevation per capire quale modello usare (usato sopra)\n",
    "import matplotlib.pyplot as plt\n",
    "transformed_df = forest.select('Elevation')\n",
    "transformed_df.toPandas().hist()\n",
    "plt.savefig('Elevation_histogram.png')\n",
    "plt.close('all')\n",
    "\n",
    "#Sembra una ditribuzione normale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "+--------------------+\n",
      "|            selected|\n",
      "+--------------------+\n",
      "|(10,[0,1,2,3,5,6,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SELEZIONARE LE VARIABILI PIÙ PREDITTIBILI\n",
    "\n",
    "#TOP 10 variabili per predirre la classe si un'osservazione\n",
    "\n",
    "#Raggruppiamo tutte le colonne in una\n",
    "#Tranne l'ultima che è coverType ovvero il nostro obiettivo\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[0:-1]\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "\n",
    "#Selezioni migliore feature usando il chi-square test\n",
    "selector = feat.ChiSqSelector(\n",
    "    labelCol=\"CoverType\" #colonna obiettivo\n",
    "    ,numTopFeatures=10\n",
    "    ,outputCol=\"selected\" #Vettore con le prime 10 colonne\n",
    ")\n",
    "\n",
    "pipeline_sel = Pipeline(stages=[vectorAssembler,selector])\n",
    "\n",
    "(\n",
    "    pipeline_sel\n",
    "    .fit(forest)\n",
    "    .transform(forest)\n",
    "    .select(selector.getOutputCol())\n",
    "    .show(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DenseMatrix([[ 1.        ,  0.01573494, -0.24269664, ...,  0.19359464,\n",
      "               0.21261232, -0.26955378],\n",
      "             [ 0.01573494,  1.        ,  0.07872841, ...,  0.00829428,\n",
      "              -0.00586558,  0.0170798 ],\n",
      "             [-0.24269664,  0.07872841,  1.        , ...,  0.09360193,\n",
      "               0.02563691,  0.14828541],\n",
      "             ...,\n",
      "             [ 0.19359464,  0.00829428,  0.09360193, ...,  1.        ,\n",
      "              -0.01929168,  0.15566826],\n",
      "             [ 0.21261232, -0.00586558,  0.02563691, ..., -0.01929168,\n",
      "               1.        ,  0.1283513 ],\n",
      "             [-0.26955378,  0.0170798 ,  0.14828541, ...,  0.15566826,\n",
      "               0.1283513 ,  1.        ]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Wilderness_Area_CacheLaPoudre', 'Soil_type_4703',\n",
       "       'Horizontal_Distance_To_Roadways',\n",
       "       'Horizontal_Distance_To_Hydrology', 'CoverType', 'Slope',\n",
       "       'Wilderness_Area_Neota', 'Soil_type_8771', 'Soil_type_2717',\n",
       "       'Soil_type_8776'], dtype='<U34')"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "#Selezione di variabili continue, usando la correlazione\n",
    "import pyspark.ml.stat as st\n",
    "\n",
    "features_and_label = feat.VectorAssembler(\n",
    "    inputCols=forest.columns\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "\n",
    "#Correlazione\n",
    "corr = st.Correlation.corr(\n",
    "    features_and_label.transform(forest)\n",
    "    ,\"features\"\n",
    "    ,\"pearson\" #Coeff di Pearson\n",
    ")\n",
    "\n",
    "print(str(corr.collect()[0][0]))\n",
    "\n",
    "#Estraiamo le 10 colonne più correlate col il nostro label (covertype)\n",
    "\n",
    "num_of_features = 10\n",
    "cols =dict([\n",
    "    (i,e) for i, e in enumerate (forest.columns)\n",
    "])\n",
    "\n",
    "corr_matrix = corr.collect()[0][0]\n",
    "label_corr_idx = [\n",
    "    (i[0],e) for i,e in np.ndenumerate(corr_matrix.toArray()[:,0])\n",
    "][1:]\n",
    "\n",
    "label_corr_idx_sorted = sorted(\n",
    "    label_corr_idx\n",
    "    , key=lambda el: -abs(el[1])\n",
    ")\n",
    "\n",
    "features_selected = np.array([\n",
    "    cols[el[0]]\n",
    "    for el in label_corr_idx_sorted\n",
    "])[0:num_of_features]\n",
    "features_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "source": [
    "#Predire forest coverage type\n",
    "#Usando il logistic regression model\n",
    "\n",
    "#Divisione fra train e test\n",
    "forest_train, forest_test=(\n",
    "    forest.randomSplit([0.7,0.3],seed=666)\n",
    ")\n",
    "#Uniamo le colonne in una\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[0:-1]\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "#Selezioniamo le 10 feature più utili\n",
    "selector = feat.ChiSqSelector(\n",
    "    labelCol=\"CoverType\"\n",
    "    ,numTopFeatures=10\n",
    "    ,outputCol=\"selected\"\n",
    ")\n",
    "\n",
    "#Logistic regression model\n",
    "logReg_obj = cl.LogisticRegression(\n",
    "    labelCol=\"CoverType\"\n",
    "    ,featuresCol=selector.getOutputCol()\n",
    "    ,regParam=0.01\n",
    "    ,elasticNetParam=1.0\n",
    "    ,family=\"multinomial\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[vectorAssembler,selector,logReg_obj]\n",
    ")\n",
    "\n",
    "pModel=pipeline.fit(forest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.6007431705863023, 0.5996427891047714, 0.6349616462266318)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "#Testiamo il modello\n",
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "results_logReg=(\n",
    "    pModel\n",
    "    .transform(forest_test)\n",
    "    .select(\"CoverType\",\"probability\",\"prediction\")\n",
    ")\n",
    "\n",
    "evaluator = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\"\n",
    "    ,labelCol=\"CoverType\"\n",
    ")\n",
    "\n",
    "\n",
    "(\n",
    "    evaluator.evaluate(results_logReg)\n",
    "    , evaluator.evaluate(results_logReg\n",
    "    , {evaluator.metricName: 'weightedPrecision'}\n",
    ")\n",
    "    , evaluator.evaluate(\n",
    "    results_logReg\n",
    "    , {evaluator.metricName: 'accuracy'}\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/07/15 16:52:07 WARN DAGScheduler: Broadcasting large task binary with size 1624.2 KiB\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8309117285326462"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "#Stima forest elevation\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[1:] #la 1 è l'obiettivo\n",
    "    , outputCol='features'\n",
    "    )\n",
    "\n",
    "rf_obj = rg.RandomForestRegressor(\n",
    "    labelCol='Elevation'\n",
    "    , maxDepth=10\n",
    "    , minInstancesPerNode=10\n",
    "    , minInfoGain=0.1\n",
    "    , numTrees=10\n",
    "    )\n",
    "pip = Pipeline(stages=[vectorAssembler, rf_obj])\n",
    "\n",
    "#Test performance\n",
    "results = (\n",
    "pip\n",
    "    .fit(forest)\n",
    "    .transform(forest)\n",
    "    .select('Elevation', 'prediction')\n",
    ")\n",
    "evaluator = ev.RegressionEvaluator(labelCol='Elevation')\n",
    "evaluator.evaluate(results, {evaluator.metricName: 'r2'})\n",
    "\n",
    "#83% meglio della regressione lineare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "+--------------------+---------+----------+\n",
      "|            features|CoverType|prediction|\n",
      "+--------------------+---------+----------+\n",
      "|(54,[0,1,2,3,5,6,...|        5|         6|\n",
      "|(54,[0,1,2,3,4,5,...|        5|         6|\n",
      "|(54,[0,1,2,3,4,5,...|        2|         6|\n",
      "|(54,[0,1,2,3,4,5,...|        2|         6|\n",
      "|(54,[0,1,2,3,4,5,...|        5|         6|\n",
      "+--------------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MODELLO DI CLUSTERING\n",
    "import pyspark.ml.clustering as clust\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[:-1]\n",
    "    , outputCol='features'\n",
    ")\n",
    "\n",
    "kmeans_obj = clust.KMeans(k=7,seed=666) #k numero di cluster previsto\n",
    "\n",
    "pip=Pipeline(stages=[vectorAssembler,kmeans_obj])\n",
    "\n",
    "#Guardiamo i risultati\n",
    "results = (\n",
    "    pip\n",
    "    .fit(forest)\n",
    "    .transform(forest)\n",
    "    .select('features', 'CoverType', 'prediction')\n",
    ")\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5084268785313815"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "#Valutazione risultati\n",
    "clustering_ev = ev.ClusteringEvaluator()\n",
    "clustering_ev.evaluate(results)\n",
    "\n",
    "#0.50 buon risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_246256/2374559654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mdata_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m logReg_modelTest = cross_v.fit(\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mdata_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mbestIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0mbestModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbestIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossValidatorModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Tuning dei parametri\n",
    "#Meglio non eseguire questa cella, esecuzione molto lunga\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "inputCols=forest.columns[0:-1]\n",
    ", outputCol='features')\n",
    "\n",
    "selector = feat.ChiSqSelector(\n",
    "labelCol='CoverType'\n",
    ", numTopFeatures=5\n",
    ", outputCol='selected')\n",
    "\n",
    "logReg_obj = cl.LogisticRegression(\n",
    "labelCol='CoverType'\n",
    ", featuresCol=selector.getOutputCol()\n",
    ", family='multinomial'\n",
    ")\n",
    "#Tuning di 2 parametri con 2 livelli ognuno -> 4 modelli da creare\n",
    "logReg_grid = (\n",
    "    tune.ParamGridBuilder()\n",
    "    .addGrid(logReg_obj.regParam\n",
    "    , [0.01, 0.1]\n",
    "    )\n",
    "    .addGrid(logReg_obj.elasticNetParam\n",
    "    , [1.0, 0.5]\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "logReg_ev = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction'\n",
    "    , labelCol='CoverType')\n",
    "\n",
    "cross_v = tune.CrossValidator(\n",
    "    estimator=logReg_obj\n",
    "    , estimatorParamMaps=logReg_grid\n",
    "    , evaluator=logReg_ev\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, selector])\n",
    "data_trans = pipeline.fit(forest_train)\n",
    "\n",
    "logReg_modelTest = cross_v.fit(\n",
    "data_trans.transform(forest_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testiamo il modello\n",
    "data_trans_test = data_trans.transform(forest_test)\n",
    "results = logReg_modelTest.transform(data_trans_test)\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedPrecision'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedRecall'}))\n",
    "print(logReg_ev.evaluate(results, {logReg_ev.metricName: 'accuracy'}))\n",
    "\n",
    "#È peggiorata a causa della scelta di 5 feature e non 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "source": [
    "#Estrarre feauture dal testo\n",
    "import pyspark.ml.feature as feat\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Si splitta sugli spazi e infine ci contano le ricorrenze di ogni parola e attraverso un hashing portarci il variabili numeriche\n",
    "\n",
    "#Definisco dataset di frasi\n",
    "\n",
    "some_text = spark.createDataFrame([\n",
    "['''\n",
    "Apache Spark achieves high performance for both batch\n",
    "and streaming data, using a state-of-the-art DAG scheduler,\n",
    "a query optimizer, and a physical execution engine.\n",
    "''']\n",
    ", ['''\n",
    "Apache Spark is a fast and general-purpose cluster computing\n",
    "system. It provides high-level APIs in Java, Scala, Python\n",
    "and R, and an optimized engine that supports general execution\n",
    "graphs. It also supports a rich set of higher-level tools including\n",
    "Spark SQL for SQL and structured data processing, MLlib for machine\n",
    "learning, GraphX for graph processing, and Spark Streaming.\n",
    "''']\n",
    ", ['''\n",
    "Machine learning is a field of computer science that often uses\n",
    "statistical techniques to give computers the ability to \"learn\"\n",
    "(i.e., progressively improve performance on a specific task)\n",
    "with data, without being explicitly programmed.\n",
    "''']\n",
    "], ['text'])\n",
    "\n",
    "\n",
    "#Splitto usando spazi , . \\ \"\n",
    "splitter = feat.RegexTokenizer(\n",
    "    inputCol='text'\n",
    "    , outputCol='text_split'\n",
    "    , pattern='\\s+|[,.\\\"]'\n",
    ")\n",
    "\n",
    "#Rimuovo parole non significative (stop-word)\n",
    "sw_remover = feat.StopWordsRemover(\n",
    "    inputCol=splitter.getOutputCol()\n",
    "    , outputCol='no_stopWords'\n",
    ")\n",
    "\n",
    "#Hashing\n",
    "hasher = feat.HashingTF(\n",
    "    inputCol=sw_remover.getOutputCol()\n",
    "    , outputCol='hashed'\n",
    "    , numFeatures=20\n",
    ")\n",
    "\n",
    "#frequenty-inverse document frequency\n",
    "#frequenza della parola nel testo / in tutti i testi\n",
    "#Misura l'importanza della parola nel testo\n",
    "idf = feat.IDF(\n",
    "    inputCol=hasher.getOutputCol()\n",
    "    , outputCol='features'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[splitter, sw_remover, hasher, idf])\n",
    "pipelineModel = pipeline.fit(some_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disretizzazione variabili continue\n",
    "\n",
    "#Usiamo un DF diverso da prima\n",
    "signal_df = spark.read.csv(\n",
    "'./learningPySpark/Data/fourier_signal.csv'\n",
    ", header=True\n",
    ", inferSchema=True\n",
    ")\n",
    "\n",
    "#Discretizzatore\n",
    "steps = feat.QuantileDiscretizer(\n",
    "    numBuckets=10,\n",
    "    inputCol='signal',\n",
    "    outputCol='discretized')\n",
    "\n",
    "#DF trasformato\n",
    "transformed = (\n",
    "    steps\n",
    "    .fit(signal_df)\n",
    "    .transform(signal_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizzazione variabili continue\n",
    "\n",
    "#Rappresentazione vettorizzata\n",
    "vec = feat.VectorAssembler(\n",
    "    inputCols=['signal']\n",
    "    , outputCol='signal_vec'\n",
    ")\n",
    "\n",
    "norm = feat.StandardScaler( #accetta solo Rappresentazione vettorizzata\n",
    "    inputCol=vec.getOutputCol()\n",
    "    , outputCol='signal_norm'\n",
    "    , withMean=True #=0\n",
    "    , withStd=True #=1\n",
    ")\n",
    "\n",
    "norm_pipeline = Pipeline(stages=[vec, norm])\n",
    "signal_norm = (\n",
    "    norm_pipeline\n",
    "    .fit(signal_df)\n",
    "    .transform(signal_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<>:4: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n<>:4: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n/tmp/ipykernel_19699/3491647991.py:4: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n  ('''\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19699/3491647991.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m articles = spark.createDataFrame([\n\u001b[0;32m----> 4\u001b[0;31m ('''\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mAndromeda\u001b[0m \u001b[0mGalaxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamed\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmythological\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mPrincess\u001b[0m \u001b[0mAndromeda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malso\u001b[0m \u001b[0mknown\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMessier\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM31\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "#Divisione in bucket in base alle parole\n",
    "\n",
    "articles = spark.createDataFrame([\n",
    "('''\n",
    "    The Andromeda Galaxy, named after the mythological\n",
    "    Princess Andromeda, also known as Messier 31, M31,\n",
    "    or NGC 224, is a spiral galaxy approximately 780\n",
    "    kiloparsecs (2.5 million light-years) from Earth,\n",
    "    and the nearest major galaxy to the Milky Way.\n",
    "    Its name stems from the area of the sky in which it\n",
    "    appears, the constellation of Andromeda. The 2006\n",
    "    observations by the Spitzer Space Telescope revealed\n",
    "    that the Andromeda Galaxy contains approximately one\n",
    "    trillion stars, more than twice the number of the\n",
    "    Milky Way’s estimated 200-400 billion stars. The\n",
    "    Andromeda Galaxy, spanning approximately 220,000 light\n",
    "    years, is the largest galaxy in our Local Group,\n",
    "    which is also home to the Triangulum Galaxy and\n",
    "    other minor galaxies. The Andromeda Galaxy's mass is\n",
    "    estimated to be around 1.76 times that of the Milky\n",
    "    Way Galaxy (~0.8-1.5×1012 solar masses vs the Milky\n",
    "    Way's 8.5×1011 solar masses).\n",
    "    ''','Galaxy', 'Andromeda')\n",
    "    (...)\n",
    ", ('''\n",
    "Washington, officially the State of Washington, is a state in the Pacific\n",
    "Northwest region of the United States. Named after George Washington,\n",
    "the first president of the United States, the state was made out of the\n",
    "western part of the Washington Territory, which was ceded by Britain in\n",
    "1846 in accordance with the Oregon Treaty in the settlement of the\n",
    "Oregon boundary dispute. It was admitted to the Union as the 42nd state\n",
    "in 1889. Olympia is the state capital. Washington is sometimes referred\n",
    "to as Washington State, to distinguish it from Washington, D.C., the\n",
    "capital of the United States, which is often shortened to Washington.\n",
    "''','Geography', 'Washington State')\n",
    "], ['articles', 'Topic', 'Object'])\n",
    "\n",
    "#Come visto prima\n",
    "\n",
    "splitter = feat.RegexTokenizer(\n",
    "inputCol='articles'\n",
    ", outputCol='articles_split'\n",
    ", pattern='\\s+|[,.\\\"]'\n",
    ")\n",
    "sw_remover = feat.StopWordsRemover(\n",
    "inputCol=splitter.getOutputCol()\n",
    ", outputCol='no_stopWords'\n",
    ")\n",
    "count_vec = feat.CountVectorizer(inputCol=sw_remover.getOutputCol()\n",
    ", outputCol='vector'\n",
    ")\n",
    "\n",
    "#Clusterizzazione\n",
    "#Latent dirichlet allocation\n",
    "lda_clusters = clust.LDA(\n",
    "    k=3 #ci aspettimo 3 cluster\n",
    "    , optimizer='online'\n",
    "    , featuresCol=count_vec.getOutputCol()\n",
    ")\n",
    "\n",
    "topic_pipeline = Pipeline(\n",
    "    stages=[\n",
    "    splitter\n",
    "    , sw_remover\n",
    "    , count_vec\n",
    "    , lda_clusters\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'topic_pipeline' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19699/2952536556.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m for topic in (\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtopic_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "for topic in (\n",
    "    topic_pipeline\n",
    "    .fit(articles)\n",
    "    .transform(articles)\n",
    "    .select('Topic','Object','topicDistribution')\n",
    "    .take(10)\n",
    "):\n",
    "    print(\n",
    "    topic.Topic\n",
    "    , topic.Object\n",
    "    , np.argmax(topic.topicDistribution)\n",
    "    , topic.topicDistribution\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}