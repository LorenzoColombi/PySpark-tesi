{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/07/14 20:59:06 WARN Utils: Your hostname, lorenzo-X555UJ resolves to a loopback address: 127.0.1.1; using 192.168.43.245 instead (on interface wlp3s0)\n",
      "21/07/14 20:59:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/07/14 20:59:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#INIZIALIZZAIZIONE SESSION E AVVIO SPARK\n",
    "import pyspark as pys \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql as sql\n",
    "\n",
    "#Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Spark context\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_Rawah: integer (nullable = true)\n",
      " |-- Wilderness_Area_Neota: integer (nullable = true)\n",
      " |-- Wilderness_Area_Comanche: integer (nullable = true)\n",
      " |-- Wilderness_Area_CacheLaPoudre: integer (nullable = true)\n",
      " |-- Soil_type_2702: integer (nullable = true)\n",
      " |-- Soil_type_2703: integer (nullable = true)\n",
      " |-- Soil_type_2704: integer (nullable = true)\n",
      " |-- Soil_type_2705: integer (nullable = true)\n",
      " |-- Soil_type_2706: integer (nullable = true)\n",
      " |-- Soil_type_2717: integer (nullable = true)\n",
      " |-- Soil_type_3501: integer (nullable = true)\n",
      " |-- Soil_type_3502: integer (nullable = true)\n",
      " |-- Soil_type_4201: integer (nullable = true)\n",
      " |-- Soil_type_4703: integer (nullable = true)\n",
      " |-- Soil_type_4704: integer (nullable = true)\n",
      " |-- Soil_type_4744: integer (nullable = true)\n",
      " |-- Soil_type_4758: integer (nullable = true)\n",
      " |-- Soil_type_5101: integer (nullable = true)\n",
      " |-- Soil_type_5151: integer (nullable = true)\n",
      " |-- Soil_type_6101: integer (nullable = true)\n",
      " |-- Soil_type_6102: integer (nullable = true)\n",
      " |-- Soil_type_6731: integer (nullable = true)\n",
      " |-- Soil_type_7101: integer (nullable = true)\n",
      " |-- Soil_type_7102: integer (nullable = true)\n",
      " |-- Soil_type_7103: integer (nullable = true)\n",
      " |-- Soil_type_7201: integer (nullable = true)\n",
      " |-- Soil_type_7202: integer (nullable = true)\n",
      " |-- Soil_type_7700: integer (nullable = true)\n",
      " |-- Soil_type_7701: integer (nullable = true)\n",
      " |-- Soil_type_7702: integer (nullable = true)\n",
      " |-- Soil_type_7709: integer (nullable = true)\n",
      " |-- Soil_type_7710: integer (nullable = true)\n",
      " |-- Soil_type_7745: integer (nullable = true)\n",
      " |-- Soil_type_7746: integer (nullable = true)\n",
      " |-- Soil_type_7755: integer (nullable = true)\n",
      " |-- Soil_type_7756: integer (nullable = true)\n",
      " |-- Soil_type_7757: integer (nullable = true)\n",
      " |-- Soil_type_7790: integer (nullable = true)\n",
      " |-- Soil_type_8703: integer (nullable = true)\n",
      " |-- Soil_type_8707: integer (nullable = true)\n",
      " |-- Soil_type_8708: integer (nullable = true)\n",
      " |-- Soil_type_8771: integer (nullable = true)\n",
      " |-- Soil_type_8772: integer (nullable = true)\n",
      " |-- Soil_type_8776: integer (nullable = true)\n",
      " |-- CoverType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CARICAMENTO DF\n",
    "\n",
    "forest_path = './learningPySpark/Data/forest_coverage_type.csv'\n",
    "\n",
    "forest=spark.read.csv(\n",
    "    forest_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "forest.printSchema()\n",
    "#Sono tutti campi numerici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "+--------------------------------+------------------------------------+\n",
      "|Horizontal_Distance_To_Hydrology|Horizontal_Distance_To_Hydrology_Bkt|\n",
      "+--------------------------------+------------------------------------+\n",
      "|                             258|                                 2.0|\n",
      "|                             212|                                 1.0|\n",
      "|                             268|                                 2.0|\n",
      "|                             242|                                 1.0|\n",
      "|                             153|                                 1.0|\n",
      "+--------------------------------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Transformers\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.ml.feature as feat\n",
    "import numpy as np\n",
    "\n",
    "#Bucketizer\n",
    "bucket_no=10 \n",
    "\n",
    "dist_min_max = (\n",
    "    forest.agg( #prendo min e max\n",
    "        f.min('Horizontal_Distance_To_Hydrology').alias(\"min\")\n",
    "        ,f.max('Horizontal_Distance_To_Hydrology').alias(\"max\")\n",
    "    ).rdd\n",
    "    .map(lambda row: (row.min,row.max))\n",
    "    .collect()[0] #Non prenso la lista ma solo il primo (unico) elemento\n",
    "\n",
    ")\n",
    "#range\n",
    "rng = dist_min_max[1]-dist_min_max[0]\n",
    "\n",
    "#Prendo 11 punti equidistanti nel range\n",
    "#Valori limite di ogni bucket (11 tagli = 10 bucket)\n",
    "#Nei tagli contano anche gli estrmi\n",
    "splits = list(np.arange(\n",
    "    dist_min_max[0],\n",
    "    dist_min_max[1],\n",
    "    rng/ (bucket_no+1)\n",
    ")\n",
    ")\n",
    "\n",
    "#Creazione oggetto\n",
    "bucketizer = feat.Bucketizer(\n",
    "    splits=splits, #array con valori in cui tagliare\n",
    "    inputCol=\"Horizontal_Distance_To_Hydrology\",\n",
    "    outputCol=\"Horizontal_Distance_To_Hydrology_Bkt\"\n",
    ")\n",
    "\n",
    "#Trasformazione\n",
    "(\n",
    "    bucketizer.transform(forest).select(\n",
    "        'Horizontal_Distance_To_Hydrology'\n",
    "        ,'Horizontal_Distance_To_Hydrology_Bkt'\n",
    "    ).show(5)\n",
    ")\n",
    "\n",
    "#DF con 2 colonne, il valori continuo e il numero di bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/07/14 21:00:48 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "21/07/14 21:00:48 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "21/07/14 21:00:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(feat=SparseVector(55, {0: 2596.0, 1: 51.0, 2: 3.0, 3: 258.0, 5: 510.0, 6: 221.0, 7: 232.0, 8: 148.0, 9: 6279.0, 10: 1.0, 42: 1.0, 54: 5.0}), pca_feat=DenseVector([-3887.7711, 4996.8103, 2323.0932, 1014.5873, -135.1702]))]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#Gli estimators (vedi sotto) vogliono una sola colonna\n",
    "#Effettuiamo in raggrupamento\n",
    "\n",
    "#raggruppiamo tutte e colonne nella colonna feat\n",
    "vectorAssembler = (\n",
    "    feat.VectorAssembler(inputCols=forest.columns,outputCol='feat')\n",
    ")\n",
    "\n",
    "#Prendiamo le 5 variabili (feature) pi√π significative\n",
    "pca = (\n",
    "    feat.PCA(k=5,inputCol=vectorAssembler.getOutputCol(),outputCol='pca_feat')\n",
    ")\n",
    "\n",
    "(\n",
    "    pca\n",
    "    .fit(vectorAssembler.transform(forest))\n",
    "    .transform(vectorAssembler.transform(forest))\n",
    "    .select(\"feat\",\"pca_feat\")\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/07/14 21:12:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/07/14 21:12:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "#ESTIMATORS\n",
    "import pyspark.ml.classification as cl\n",
    "\n",
    "#Cerchiamo di predirre quando la foresta avr√† \"cover type\" = 1 (spruce-fir conifere)\n",
    "\n",
    "#Creiamo un SVM MODEL\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[0:-1] #selezioniamo tutte le colonne tranne l'ultima (cover type)\n",
    "    , outputCol=\"features\"\n",
    ")\n",
    "\n",
    "fir_dataset = (\n",
    "    vectorAssembler.transform(forest).withColumn( #aggiunge colonne\n",
    "        # (NomeColonna,Dati)\n",
    "        \"label\"\n",
    "        ,(f.col(\"CoverType\")==1).cast(\"integer\") #1 = conifera\n",
    "    )\n",
    "    .select(\"label\",\"features\")\n",
    ")\n",
    "\n",
    "svc_obj = cl.LinearSVC(maxIter=10,regParam=0.01)\n",
    "svc_model = svc_obj.fit(fir_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DenseVector([-0.0001, -0.0, -0.0023, -0.0, -0.0001, 0.0, -0.001, -0.0017, -0.0003, -0.0, 0.0, 0.0401, -0.0071, -0.0958, -0.0901, -0.0653, -0.0655, -0.0437, -0.0928, -0.0848, -0.0211, -0.0045, -0.0498, -0.0829, -0.0522, -0.0325, -0.0263, -0.0923, -0.0889, -0.0275, -0.0606, -0.0595, 0.0341, -0.003, 0.0822, 0.0607, 0.0351, 0.0093, 0.0048, -0.0154, 0.0422, -0.0673, -0.0039, -0.0142, 0.0036, 0.0078, 0.0, -0.0117, 0.0283, -0.0002, -0.0463, 0.0394, 0.0292, 0.0358])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#Estraiamo il risultato della modellizazione\n",
    "svc_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "source": [
    "#Regressione lineare\n",
    "import pyspark.ml.regression as rg\n",
    "\n",
    "#Cerchiamo di stimare l'altezza di una foresta\n",
    "\n",
    "vectorAssembler = feat.VectorAssembler(\n",
    "    inputCols=forest.columns[1:]\n",
    "    ,outputCol=\"features\"\n",
    ")\n",
    "\n",
    "elevation_dataset = (\n",
    "    vectorAssembler.transform(forest).withColumn( #aggiunge colonne\n",
    "        \"label\",\n",
    "        f.col(\"Elevation\").cast(\"float\")#regressione con variabili continue\n",
    "    )\n",
    "    .select(\"label\",\"features\")\n",
    ")\n",
    "\n",
    "#Modello regressione lineare\n",
    "lr_obj = rg.LinearRegression(maxIter=10,regParam=0.01,elasticNetParam=1.00)\n",
    "lr_model=lr_obj.fit(elevation_dataset)\n",
    "\n",
    "#Il codice per creare un modello √® abbastanza standard √® di facile realizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7860412464754158 129.50871925702575 103.34079732698777\n"
     ]
    }
   ],
   "source": [
    "#Risultati della modellazione\n",
    "lr_model.coefficients\n",
    "\n",
    "#Resoconto automatico\n",
    "summary = lr_model.summary\n",
    "\n",
    "print(\n",
    "    summary.r2 #78% buon risultato\n",
    "    , summary.rootMeanSquaredError\n",
    "    ,summary.meanAbsoluteError\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7860412464754158 129.50871925702575 103.34079732698777\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}