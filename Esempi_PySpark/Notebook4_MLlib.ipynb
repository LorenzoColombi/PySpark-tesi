{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INIZIALIZZAIZIONE SESSION E AVVIO SPARK\n",
    "import pyspark as pys \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql as sql\n",
    "import pyspark.sql.functions as func\n",
    "#Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#Alcuni moduli sono deprecati o spostati nel ML module che vedremo in seguito\n",
    "\n",
    "#Caricamento dataset\n",
    "census_path = './learningPySpark/Data/census_income.csv'\n",
    "\n",
    "census=spark.read.csv(\n",
    "    census_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "census.take(5)\n",
    "\n",
    "census.printSchema()\n",
    "#Lo schema è corretto\n",
    "\n",
    "census.take(5)\n",
    "#È necessario rimuovere gli spazi all'inizio e alla fine delle stringhe\n",
    "#dtype lista di coppie (NomeColonna,Tipo)\n",
    "#ltrim rimuove gli spazi all'inizio\n",
    "#rtrim alla fine \n",
    "for col,typ in census.dtypes:\n",
    "    if typ == \"string\":\n",
    "        census = census.withColumn(\n",
    "            col, func.ltrim(func.rtrim(census[col]))\n",
    "        )\n",
    "census.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparazione Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PULIZIA DATASET\n",
    "\n",
    "#Selezioniamo le colonne di nostro interesse\n",
    "cols_to_keep = census.dtypes #lista di coppie (nome colonna, tipo)\n",
    "\n",
    "cols_to_keep=(\n",
    "    [\"label\",\"age\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"] +\n",
    "    [\n",
    "        #:-1 tranne ultimo elemento. label già presente nella lista sopra\n",
    "        e[0] for e in cols_to_keep[:-1] if e[1]==\"string\" #tutte le stringhe\n",
    "    ]\n",
    ")\n",
    "#Prenso il subset create dalle colonne di interesse\n",
    "census_subset = census.select(cols_to_keep)\n",
    "\n",
    "#Prendo le colonne con valori numerici di interesse\n",
    "cols_num = [\n",
    "    e[0] for e in census_subset.dtypes if e[1]==\"int\"\n",
    "]\n",
    "#Prenso le colonne con valori stringa\n",
    "cols_cat = [\n",
    "    e[0] for e in census_subset.dtypes[1:] if e[1]==\"string\" #1: non voglio label che è particolare\n",
    "    #lo trattiamo dopo\n",
    "]\n",
    "\n",
    "cols_num,cols_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTRIBUTI NUMERICI\n",
    "\n",
    "#Calcolo alcune statistiche su colonne numeriche\n",
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "rdd_num = (\n",
    "    census_subset.select(cols_num).rdd\n",
    "    .map(lambda row: [e for e in row]) #da Row a lista\n",
    ")\n",
    "\n",
    "stats_num =st.Statistics.colStats(rdd_num)\n",
    "\n",
    "for col, min_, mean_, max_, var_ in zip(\n",
    "cols_num\n",
    ", stats_num.min()\n",
    ", stats_num.mean()\n",
    ", stats_num.max()\n",
    ", stats_num.variance()\n",
    "):\n",
    "    print('{0}: min->{1:.1f}, mean->{2:.1f}, max->{3:.1f}, stdev->{4:.1f}'.format(col, min_, mean_, max_, np.sqrt(var_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STRINGHE\n",
    "#Colonne categorizzate. Contiamo le occorrenze\n",
    "\n",
    "rdd_cat =(\n",
    "    census_subset.select(cols_cat+['label']).rdd\n",
    "    .map(lambda row: [e for e in row])\n",
    ")\n",
    "\n",
    "results_cat={} #dizionario\n",
    "#Metto nel dizionario per ogni colonna una coppia (nome,numero ricorrenze)\n",
    "\n",
    "for i,col in enumerate(cols_cat+ ['label']):\n",
    "    results_cat[col] = (\n",
    "        rdd_cat.groupBy(lambda row: row[i])\n",
    "        .map(lambda el: (el[0],len(el[1])))\n",
    "        #el[0] nome attributo (raggruppato)\n",
    "        #el[1] lista valori per attributo raggruppato\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "#Stampo \n",
    "for k in results_cat:\n",
    "    print(k,sorted(\n",
    "        results_cat[k]\n",
    "        ,key=lambda el: el[1] #el[1] numero ricorrenze come elemento su cui ordinare\n",
    "        ,reverse=True)\n",
    "        ,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCOLO MATRICE DI CORRELAZIONE\n",
    "correlations = st.Statistics.corr(rdd_num) #Ritorna un array numpy (matrice di correlazione)\n",
    "print(correlations) #mancano i nomi delle colonne\n",
    "\n",
    "#Stampo la matrice (solo valori con abs()>0.05)\n",
    "for i,el_i in enumerate(abs(correlations)>0.05): #ritorna matrice di booleani\n",
    "    print(cols_num[i])\n",
    "\n",
    "    for j,el_j in (enumerate(el_i)): #el_j è un booleano\n",
    "        if el_j and j!= i:\n",
    "            print(\" \",cols_num[j],correlations[i][j])\n",
    "    print(\"\\n\") #riga vuota\n",
    "\n",
    "#Poca correlazione. Bene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test statistici\n",
    "\n",
    "#Calcoliamo l'indipendenza fra label e occupation\n",
    "\n",
    "import pyspark.mllib.linalg as ln #algebra lineare\n",
    "\n",
    "census_occupation = (\n",
    "    census.groupby(\"occupation\")\n",
    "    .pivot(\"label\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "census_occupation_coll = (\n",
    "    census_occupation.rdd.map(lambda row: (row[1:]))\n",
    "    .flatMap(lambda row: row).collect()\n",
    ")\n",
    "\n",
    "len_row = census_occupation.count()\n",
    "#DenseMatrix (numeroRighe, NColonne, Dati, Trasposta)\n",
    "dense_mat = ln.DenseMatrix(\n",
    "    len_row, 2, census_occupation_coll,True\n",
    ")\n",
    "\n",
    "chi_sq = st.Statistics.chiSqTest(dense_mat)\n",
    "\n",
    "print(chi_sq.pValue) #=0 Le occorrenze sono indipendenti\n",
    "print(dense_mat)\n",
    "\n",
    "dense_mat.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valori stringa categorizzati\n",
    "import pyspark.mllib.feature as feat\n",
    "\n",
    "#Conto il numero di valori distinti per colonna\n",
    "len_ftrs =[] #lista (colonna,numeroValoriDistinti)\n",
    "for col in cols_cat:\n",
    "    (\n",
    "        len_ftrs.append(\n",
    "            (col,census.select(col).distinct().count())\n",
    "        )\n",
    "    )\n",
    "len_ftrs=dict(len_ftrs)\n",
    "\n",
    "#Codifica dei dati\n",
    "final_data = (\n",
    "    census\n",
    "    .select(cols_to_keep)\n",
    "    .rdd\n",
    "    .map(lambda row: [\n",
    "        list(\n",
    "            feat.HashingTF(int(len_ftrs[col] / 2.0))\n",
    "            .transform(row[i])\n",
    "            .toArray()\n",
    "        ) if i >= 5\n",
    "        else [row[i]] \n",
    "        for i, col in enumerate(cols_to_keep)]\n",
    "    )\n",
    ")\n",
    "\n",
    "final_data.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codifica label (2 valori possibili)\n",
    "\n",
    "\n",
    "def labelEncode(label):\n",
    "    return [int(label[0] == '>50K')]\n",
    "\n",
    "final_data = (\n",
    "    final_data\n",
    "    .map(lambda row: labelEncode(row[0]) \n",
    "         + [item \n",
    "            for sublist in row[1:] \n",
    "            for item in sublist]\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0,\n",
       "  DenseVector([0.0307, 0.1485, -0.2167, -0.0354, -1.2635, 0.008, 1.7796, 1.0001, 0.83, 0.5743, -0.3473, -0.443, 0.6826, -0.4007, -0.3862, -0.4685, -1.1369, -0.4555, 0.4551, -1.1329, 2.0776, 1.8713, -1.0381, -0.3381, -0.2381, -0.775, 0.9805, 1.5207, 0.3083, -0.0634, -0.2574, -0.7031, 0.3208, -0.0901, -0.1263, 0.3355, -0.1223, 0.3378, -0.0853, -0.0937, -0.0887, -0.2104, 0.0, 0.1286, -0.1976, -0.1433, -0.1419, 0.1895, 0.298, 0.2896, 0.1638, 0.1221, 0.0])),\n",
       " (0,\n",
       "  DenseVector([-0.7024, -0.1459, -0.2167, 1.4224, -0.2368, 0.008, -0.5593, -0.2503, 0.83, 1.3423, -0.3473, -0.443, 0.6826, -0.4007, -0.3862, 1.1159, -0.4505, -1.1655, -2.1192, -0.0989, -0.8052, 0.6842, 1.0935, -0.3381, -0.2381, -0.775, 0.9805, 1.5207, 0.3083, -0.0634, -0.2574, -0.7031, 0.3208, -0.0901, -0.1263, 0.3355, -0.1223, 0.3378, -0.0853, -0.0937, -0.0887, -0.2104, 0.0, 0.1286, -0.1976, -0.1433, -0.1419, 0.1895, 0.298, 0.2896, 0.1638, 0.1221, 0.0])),\n",
       " (0,\n",
       "  DenseVector([-0.2626, -0.1459, -0.2167, -0.0354, -2.2902, -2.7619, 1.7796, -2.126, -0.2076, -0.1937, -0.3473, -0.443, -0.9076, -0.4007, 1.1933, -0.4685, 0.9222, 0.9646, 0.4551, -1.1329, -0.8052, -0.5029, -1.7486, -2.1857, -1.306, -0.775, 0.1491, -0.5788, -0.8596, -0.0634, -0.2574, -0.7031, 0.3208, -0.0901, -0.1263, 0.3355, -0.1223, 0.3378, -0.0853, -0.0937, -0.0887, -0.2104, 0.0, 0.1286, -0.1976, -0.1433, -0.1419, 0.1895, 0.298, 0.2896, 0.1638, 0.1221, 0.0]))]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "#NORMALIZZAZIONE (standardizzazione)\n",
    "\n",
    "#StandardScaler(Media,Stddev) standardizzazione z-score\n",
    "\n",
    "standardizer = feat.StandardScaler(True, True)\n",
    "sModel = standardizer.fit(final_data.map(lambda row: row[1:]))\n",
    "final_data_scaled = sModel.transform(final_data.map(lambda row: row[1:]))\n",
    "\n",
    "final_data = (\n",
    "    final_data\n",
    "    .map(lambda row: row[0])\n",
    "    .zipWithIndex()\n",
    "    .map(lambda row: (row[1], row[0]))\n",
    "    .join(\n",
    "        final_data_scaled\n",
    "        .zipWithIndex()\n",
    "        .map(lambda row: (row[1], row[0]))\n",
    "    )\n",
    "    .map(lambda row: row[1])\n",
    ")\n",
    "\n",
    "final_data.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0,\n",
       "  DenseVector([0.0307, 0.1485, -0.2167, -0.0354, -1.2635, 0.008, 1.7796, 1.0001, 0.83, 0.5743, -0.3473, -0.443, 0.6826, -0.4007, -0.3862, -0.4685, -1.1369, -0.4555, 0.4551, -1.1329, 2.0776, 1.8713, -1.0381, -0.3381, -0.2381, -0.775, 0.9805, 1.5207, 0.3083, -0.0634, -0.2574, -0.7031, 0.3208, -0.0901, -0.1263, 0.3355, -0.1223, 0.3378, -0.0853, -0.0937, -0.0887, -0.2104, 0.0, 0.1286, -0.1976, -0.1433, -0.1419, 0.1895, 0.298, 0.2896, 0.1638, 0.1221, 0.0]))]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "final_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(DenseVector([0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0]),\n",
       " DenseVector([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]))"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "sModel.mean, sModel.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ""
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.030670086380022974,0.14845061558793732,-0.21665620002803668,-0.03542890292131961,-1.263495506961592,0.007996473732139464,1.7795863357831083,1.000120836900974,0.8300118030815629,0.5743180912245661,-0.34730116635185726,-0.4429929605624822,0.6826214082675929,-0.40070785156170013,-0.3862275949058749,-0.46854980553952685,-1.1368735829205499,-0.4555118517327585,0.45507885276723214,-1.1329160643506926,2.0775754968328806,1.8712975153274591,-1.0380868217125387,-0.33808094251059423,-0.23812708304818486,-0.7750139160707569,0.9804554188515354,1.5207190561281538,0.30834093641720645,-0.06339046129248913,-0.25736785057582295,-0.7030605487269814,0.3207798471547668,-0.09006362818252117,-0.12626697150459493,0.33547741813187393,-0.12230357893453815,0.33781064530673466,-0.08526116800202502,-0.0936707494514088,-0.08866697117953638,-0.21041541274675832,0.0,0.12864878811001434,-0.19757081643280744,-0.1433270085322855,-0.1419322155844281,0.18951895277276848,0.2979804796353552,0.2896265911948923,0.16375112206978876,0.12209582618287587,0.0]),\n",
       " LabeledPoint(0.0, [-0.7024444949002066,-0.14591824281680113,-0.21665620002803668,1.4223644939039815,-0.2368265809739598,0.007996473732139464,-0.55926777842209,-0.25033263608866274,0.8300118030815629,1.3423320288960559,-0.34730116635185726,-0.4429929605624822,0.6826214082675929,-0.40070785156170013,-0.3862275949058749,1.1158774681308994,-0.4505209902429061,-1.165546865580733,-2.119247506675757,-0.09894796245527243,-0.8052309359795043,0.6842235727885942,1.0934913125964225,-0.33808094251059423,-0.23812708304818486,-0.7750139160707569,0.9804554188515354,1.5207190561281538,0.30834093641720645,-0.06339046129248913,-0.25736785057582295,-0.7030605487269814,0.3207798471547668,-0.09006362818252117,-0.12626697150459493,0.33547741813187393,-0.12230357893453815,0.33781064530673466,-0.08526116800202502,-0.0936707494514088,-0.08866697117953638,-0.21041541274675832,0.0,0.12864878811001434,-0.19757081643280744,-0.1433270085322855,-0.1419322155844281,0.18951895277276848,0.2979804796353552,0.2896265911948923,0.16375112206978876,0.12209582618287587,0.0])]"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "import pyspark.mllib.regression as reg\n",
    "#Creazione labeled points per la classificazione\n",
    "final_data_income = (\n",
    "    final_data\n",
    "    .map(lambda row: reg.LabeledPoint(\n",
    "        row[0]\n",
    "        , row[1]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "final_data_income.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[LabeledPoint(-0.03542890292131962, [0.0,0.030670086380022974,0.14845061558793732,-0.21665620002803668,-1.263495506961592,0.007996473732139464,1.7795863357831083,1.000120836900974,0.8300118030815629,0.5743180912245661,-0.34730116635185726,-0.4429929605624822,0.6826214082675929,-0.40070785156170013,-0.3862275949058749,-0.46854980553952685,-1.1368735829205499,-0.4555118517327585,0.45507885276723214,-1.1329160643506926,2.0775754968328806,1.8712975153274591,-1.0380868217125387,-0.33808094251059423,-0.23812708304818486,-0.7750139160707569,0.9804554188515354,1.5207190561281538,0.30834093641720645,-0.06339046129248913,-0.25736785057582295,-0.7030605487269814,0.3207798471547668,-0.09006362818252117,-0.12626697150459493,0.33547741813187393,-0.12230357893453815,0.33781064530673466,-0.08526116800202502,-0.0936707494514088,-0.08866697117953638,-0.21041541274675832,0.0,0.12864878811001434,-0.19757081643280744,-0.1433270085322855,-0.1419322155844281,0.18951895277276848,0.2979804796353552,0.2896265911948923,0.16375112206978876,0.12209582618287587,0.0]),\n",
       " LabeledPoint(1.4223644939039817, [0.0,-0.7024444949002066,-0.14591824281680113,-0.21665620002803668,-0.2368265809739598,0.007996473732139464,-0.55926777842209,-0.25033263608866274,0.8300118030815629,1.3423320288960559,-0.34730116635185726,-0.4429929605624822,0.6826214082675929,-0.40070785156170013,-0.3862275949058749,1.1158774681308994,-0.4505209902429061,-1.165546865580733,-2.119247506675757,-0.09894796245527243,-0.8052309359795043,0.6842235727885942,1.0934913125964225,-0.33808094251059423,-0.23812708304818486,-0.7750139160707569,0.9804554188515354,1.5207190561281538,0.30834093641720645,-0.06339046129248913,-0.25736785057582295,-0.7030605487269814,0.3207798471547668,-0.09006362818252117,-0.12626697150459493,0.33547741813187393,-0.12230357893453815,0.33781064530673466,-0.08526116800202502,-0.0936707494514088,-0.08866697117953638,-0.21041541274675832,0.0,0.12864878811001434,-0.19757081643280744,-0.1433270085322855,-0.1419322155844281,0.18951895277276848,0.2979804796353552,0.2896265911948923,0.16375112206978876,0.12209582618287587,0.0])]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "#RDD di labeled points per la regressione\n",
    "\n",
    "mu, std = sModel.mean[3], sModel.std[3]\n",
    "\n",
    "final_data_hours = (\n",
    "    final_data\n",
    "    .map(lambda row: reg.LabeledPoint(\n",
    "        row[1][3] * std + mu\n",
    "        , ln.Vectors.dense([row[0]] + list(row[1][0:3]) + list(row[1][4:]))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "final_data_hours.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividiamo il dataset in training e testing\n",
    "\n",
    "(\n",
    "    final_data_income_train\n",
    "    , final_data_income_test\n",
    "    ) = (\n",
    "    #Come parametro la proporzione da assegnare ad ogni dataset\n",
    "    final_data_income.randomSplit([0.7, 0.3])\n",
    ")\n",
    "\n",
    "(\n",
    "    final_data_hours_train\n",
    "    , final_data_hours_test\n",
    "    ) = (\n",
    "    final_data_hours.randomSplit([0.7, 0.3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1.4223644939039817 0.10542444909923396\n",
      "-0.03542890292131962 0.5115554270579242\n",
      "-0.03542890292131962 -0.05563934053726413\n",
      "-0.03542890292131962 0.2857853919345268\n",
      "0.36951370730793076 0.09397650078312011\n",
      "-1.2502567336090709 -0.6734044368007291\n",
      "0.7744563175371811 0.0760273478274319\n",
      "1.1793989277664314 -0.07481547691060852\n",
      "0.36951370730793076 0.23583431166140195\n",
      "0.9364333616288814 0.5147502219910091\n"
     ]
    }
   ],
   "source": [
    "#Predire ore di lavoro\n",
    "#con regressione lineare\n",
    "\n",
    "workhours_model_lm = reg.LinearRegressionWithSGD.train(final_data_hours_train)\n",
    "\n",
    "#Testiamo il modello\n",
    "small_sample_hours = sc.parallelize(final_data_hours_test.take(10))\n",
    "\n",
    "for t,p in zip(\n",
    "    small_sample_hours\n",
    "    .map(lambda row: row.label).collect()\n",
    "    , workhours_model_lm.predict(small_sample_hours\n",
    "    .map(lambda row: row.features)\n",
    ").collect()):\n",
    "    print(t,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0.0 1\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "1.0 1\n",
      "0.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.0 0\n",
      "0.0 0\n"
     ]
    }
   ],
   "source": [
    "#Predire guadagno lavoro\n",
    "#con SVM support vector machine\n",
    "import pyspark.mllib.classification as cl\n",
    "\n",
    "income_model_lr = cl.LogisticRegressionWithSGD.train(final_data_income_train)\n",
    "\n",
    "#Testiamo il modello\n",
    "small_sample_income = sc.parallelize(final_data_income_test.take(10))\n",
    "\n",
    "for t,p in zip(\n",
    "    small_sample_income\n",
    "    .map(lambda row: row.label)\n",
    "    .collect()\n",
    "    , income_model_lr.predict(\n",
    "    small_sample_income\n",
    "    .map(lambda row: row.features)\n",
    "    ).collect()):\n",
    "        print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0.15347287281512212\n",
      "0.12233906102086833\n"
     ]
    }
   ],
   "source": [
    "#Modello di custering\n",
    "import pyspark.mllib.clustering as clu\n",
    "\n",
    "model = clu.KMeans.train(\n",
    "    final_data.map(lambda row: row[1])\n",
    "    , 2\n",
    "    , initializationMode='random'\n",
    "    , seed=666\n",
    ")\n",
    "\n",
    "import sklearn.metrics as m\n",
    "predicted = (\n",
    "model\n",
    ".predict(\n",
    "final_data.map(lambda row: row[1])\n",
    ")\n",
    ")\n",
    "predicted = predicted.collect()\n",
    "true = final_data.map(lambda row: row[0]).collect()\n",
    "print(m.homogeneity_score(true, predicted))\n",
    "print(m.completeness_score(true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MANCA CALCOLO PERFORMANCE (errori nel modello)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}